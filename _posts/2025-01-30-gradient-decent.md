---
layout: post
title: What is Gradient Descent?
date: 2024-04-26 23:18 +0800
last_modified_at: 2025-01-30 11:34:25 +0800
tags: [mathematical optimization, numpy]
math: true
toc:  true
---


# Gradient Descent?

Letâ€™s suppose you're lost somewhere in the mountains. In a situation where you donâ€™t know exactly where you are, the best way to find your way down might be to simply follow the slope. The steeper the slope, the quicker youâ€™ll reach the lowest point. This post explains an algorithm called gradient descent, which solves optimization problems, such as those in convolution functions.

>Where exactly is it used? ğŸ¤”

In machine learning, a loss function is used to calculate the difference between the predicted values and the actual values. Gradient descent is then used to find the minimum of the loss function.

---

# The Concept of Gradient Descent

## Descent


>(a) \\\(x_{k+1} = x_{k} + t_k \Delta{x_k},k=0,1,...\\\) (where \\\(t_k>0\\\))

>(b) \\\(f(x_{k+1})<f(x_k\\\))

ìœ„ì˜ ì¡°ê±´ì„ ê°€ì§„ ìˆ˜ì‹ì„ ì´ìš©í•´ ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ì‹ì„ í•˜ê°•ë²•ì´ë¼ ì •ì˜í•©ë‹ˆë‹¤. ì¦‰ $k$ê°€ ì¦ê°€í•˜ë©´ í•¨ìˆ˜ê°’ì´ ê³„ì† ê°ì†Œí•˜ë„ë¡ ìˆ˜ì—´ì„ ì„¤ì •í•˜ëŠ” ê²ƒì´ ê´€ê±´ì…ë‹ˆë‹¤.

---

## ê²½ì‚¬í•˜ê°•ë²•

ê²½ì‚¬í•˜ê°•ë²•ì— ì“°ì´ëŠ” ëª©ì í•¨ìˆ˜ëŠ” ë³¼ë¡í•¨ìˆ˜ì…ë‹ˆë‹¤. ë³¼ë¡í•¨ìˆ˜ì˜ ì •ì˜ì— ì˜í•´

\\\(f(x_{k+1}) \geq f(x_k)+\nabla f(x_k)^T(x_{k+1}-x_k)\\\)

ê°€ ì„±ë¦½í•©ë‹ˆë‹¤. ì´ë•Œ ìœ„ í•˜ê°•ë²• ì •ì˜ (b)ê°€ ì„±ë¦½í•˜ë ¤ë©´

$\nabla f(x_k)^T(x_{k+1}-x_k)<0$ì´ê³  ì´ì‹ì„ (a)ë¥¼ ì´ìš©í•´ ì •ë¦¬í•˜ë©´

$\nabla f(x_k)^T t_k\Delta{x_k}<0$ ì´ ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

$t_k$ëŠ” ì–‘ìˆ˜ì´ê¸°ì— $\Delta{x_k}$ë¥¼ ì„¤ì •í•˜ê² ìŠµë‹ˆë‹¤.

> â—ï¸ ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ $f:R^n \rightarrow R$ì´ ê°€ì¥ ë¹ ë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ì€ $\nabla f$	ë°©í–¥ì´ë‹¤.
> 

ìœ„ ì •ë¦¬ë¥¼ ê±°ê¾¸ë¡œ ë§í•˜ë©´ $\Delta{x_k}$ë¥¼  $-\nabla f(x_k)$ë¡œ ë†“ìœ¼ë©´ **ê°€ì¥ ë¹ ë¥´ê²Œ ê°ì†Œí•˜ëŠ” ë°©í–¥**ìœ¼ë¡œ $x_k$ê°€ í–¥í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ê·¸ëŸ¼ ì´ê²ƒì„ íŒŒì´ì¬ìœ¼ë¡œ êµ¬í˜„í•´ë³´ê³  ê·¸ë˜í”„ë¡œ í•œë²ˆ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.

## ğŸ§‘ğŸ»â€ğŸ’» ê²½ì‚¬í•˜ê°•ë²• ì½”ë”©í•˜ê¸°

> $f(x) = 2x^2+3xy+4y^2$ ì¼ ë•Œ, $minimize_{x \in R^2} f(x)$ë¥¼ ê²½ì‚¬í•˜ê°•ë²•ì„ ì´ìš©í•´ í’€ì–´ë³´ì„¸ìš”. (ë‹¨, $x_0=(2,4), t_k=0.01, \epsilon=10^{-8}$)
> 

```python
# ëª©ì í•¨ìˆ˜
def f(x,y):
    return 2*x**2 + 3*x*y + 4*y**2

# ëª©ì í•¨ìˆ˜ë¥¼ xë¡œ í¸ë¯¸ë¶„í•œ í•¨ìˆ˜
def fx(x,y):
    return 4*x+3*y

# ëª©ì í•¨ìˆ˜ë¥¼ yë¡œ í¸ë¯¸ë¶„í•œ í•¨ìˆ˜
def fy(x,y):
    return 3*x+8*y

# x_k,y_kì˜ ì¢Œí‘œë¥¼ ë°›ì„ ë¦¬ìŠ¤íŠ¸
xlist,ylist=[2],[4]

# ì„¤ì • ê°’
x0,y0 = 2,4
t=0.01
eps=10**(-8)

iter=0
xk,yk=x0,y0

while True:
    tk=t

    # xkp1 = xk - tk * delta(xk)
    xkp1=xk-tk*fx(xk,yk)
    ykp1=yk-tk*fy(xk,yk)

    xlist.append(xkp1)
    ylist.append(ykp1)

    if np.linalg.norm(np.array((xkp1-xk,ykp1-yk)))<eps:
        print(f'iterated {iter} times')
        print(f'GD converges to {round(xkp1,1),round(ykp1,1)}')
        break

    iter = iter + 1
    xk,yk = xkp1, ykp1

```

!https://velog.velcdn.com/images/braveveigar/post/573dd804-b6cb-4504-a08e-5d2d7d44ccd4/image.png

520ë²ˆ ê²½ì‚¬í•˜ê°•ë²•ì„ ì‹¤í–‰í•´ì„œ (0,0) ì¦‰ í•´ì— ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ§‘ğŸ»â€ğŸ’» Plotting Graph

```python
from mpl_toolkits.mplot3d import axes3d
import matplotlib.pyplot as plt
from matplotlib import animation

# x,y ê°’ì— ë”°ë¥¸ z ì¢Œí‘œ ë¦¬ìŠ¤íŠ¸
zlist=[]
for i in range(len(xlist)):
	zlist.append(f(xlist[i],ylist[i]))

# ê·¸ë˜í”„ ìƒì„±
fig = plt.figure()
ax = fig.add_subplot(111, projection='3d')
ax.view_init(elev=30, azim=-30)

# x, y ê°’ ìƒì„±
x = np.linspace(-4, 4, 10)
y = np.linspace(-4, 4, 10)
x, y = np.meshgrid(x, y)

# z ê°’ ê³„ì‚°
z = f(x,y)

# plotting 3D
ax.plot_surface(x,y,z,color='#d070fb', alpha = 0.6)
ax.plot(xlist,ylist,zlist,lw=3, color='black')
ax.text(xlist[0],ylist[0],zlist[0],'(x0,y0,f(x0,y0))')
ax.text(xlist[-1],ylist[-1],zlist[-1],'Solution')

# Labeling x, y, z
ax.set_xlabel('X-axis')
ax.set_ylabel('Y-axis')
ax.set_zlabel('Z-axis')

plt.show()

```

![alt text](image.png)

Point converges to (0,0)

---